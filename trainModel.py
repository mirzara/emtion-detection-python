# -*- coding: utf-8 -*-
"""zara.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19ncvHv_ZrKPcckRdCmWIAhI34MS9Tola
"""

!mkdir content

!unzip -q /content/archive.zip -d ./content/ckplus

!ls ./content/ckplus

import os
import cv2
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras import layers, models

def load_images(directory):
    images = []
    labels = []
    for subdir in os.listdir(directory):
        label = subdir
        for file in os.listdir(os.path.join(directory, subdir)):
            img_path = os.path.join(directory, subdir, file)
            image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  # Read image in grayscale
            image = cv2.resize(image, (48, 48))  # Resize image to 48x48
            images.append(image)
            labels.append(label)
    return np.array(images), np.array(labels)

def preprocess_data(images, labels):
    # Normalize pixel values
    images = images / 255.0
    # Split dataset into training and testing sets
    train_images, test_images, train_labels, test_labels = train_test_split(images, labels, test_size=0.2, random_state=42)
    return train_images, test_images, train_labels, test_labels

def encode_labels(train_labels, test_labels):
    label_encoder = LabelEncoder()
    train_labels_encoded = label_encoder.fit_transform(train_labels)
    test_labels_encoded = label_encoder.transform(test_labels)
    return train_labels_encoded, test_labels_encoded

# Step 6: Build the model
def build_model(input_shape, num_classes):
    model = models.Sequential([
        layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),
        layers.MaxPooling2D((2, 2)),
        layers.Conv2D(64, (3, 3), activation='relu'),
        layers.MaxPooling2D((2, 2)),
        layers.Conv2D(64, (3, 3), activation='relu'),
        layers.Flatten(),
        layers.Dense(64, activation='relu'),
        layers.Dense(num_classes, activation='softmax')
    ])
    model.compile(optimizer='adam',
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])
    return model

# Step 7: Load and preprocess images
directory = "./content/ckplus"  # Update if necessary
images, labels = load_images(directory)

# Step 8: Split the dataset
train_images, test_images, train_labels, test_labels = preprocess_data(images, labels)

# Step 9: Prepare labels
train_labels_encoded, test_labels_encoded = encode_labels(train_labels, test_labels)

# Step 10: Build the model
input_shape = (48, 48, 1)  # Image dimensions
num_classes = len(np.unique(labels))  # Number of unique emotion classes
model = build_model(input_shape, num_classes)

# Step 11: Train the model
history = model.fit(train_images, train_labels_encoded, epochs=10, batch_size=32, validation_data=(test_images, test_labels_encoded))

# Step 12: Evaluate the model
test_loss, test_acc = model.evaluate(test_images, test_labels_encoded)
print('Test accuracy:', test_acc)

# Save the trained model
model.save("/content/trained_model.h5")